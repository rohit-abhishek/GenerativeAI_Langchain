{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef08cdde",
   "metadata": {},
   "source": [
    "# Create streaming service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34fe883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83771e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b81e3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# create llms \n",
    "google_llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-3-flash-preview\")\n",
    "openai_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "ollama_llm = ChatOllama(temperature=0, model=\"ministral-3:14b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=[{'type': 'text', 'text': 'Hello! How can I help you today?', 'extras': {'signature': 'Eq4CCqsCAXLI2nwC3+r4ReVX+3ZWHzppIWXNqhQURWknHwMt+KD9ffybj/8ac0/ykuxPeFPC1kdetT4zwl0LDMOtC9GPxEE5RrQHPyS87V36N61qZGxgDsAbfiHu2WvLBNT4S+Cm+M6C6rin8z6IUrDZey0duyubEh+RlaaZfg5smTU6VQMN3lO8TXvvHb6CDIIZx/6+hLuMpgT/7p4Z8YpF3Zim6v1WMFYsgbGqNmnCLnMDGVL0Zw+tMMCFqbbNr//MmwGIQWMAqmFYj9yqk02ThFL+ItEBJhlssDwos6zT7CeepbxKRxYjIzpt/YExl4gbC3u3plwtZSGjw7Trt+cmotPD1Ngfn9Au9+InThLLsQG8psBIfZDgsK8AOCclOZSNfzToIBvAPP33anY4EUE='}}], additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-3-flash-preview', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b8916-63da-7673-b649-3cd4de645d9c-0', usage_metadata={'input_tokens': 2, 'output_tokens': 77, 'total_tokens': 79, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 68}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# google_llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44224204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! ðŸ˜Š How can I assist you today? Whether you have questions, need help with something, or just want to chat, I'm here for you!\", additional_kwargs={}, response_metadata={'model': 'ministral-3:14b', 'created_at': '2026-01-04T13:11:42.495037Z', 'done': True, 'done_reason': 'stop', 'total_duration': 36931369041, 'load_duration': 5605820041, 'prompt_eval_count': 555, 'prompt_eval_duration': 28563513875, 'eval_count': 35, 'eval_duration': 2706343670, 'logprobs': None, 'model_name': 'ministral-3:14b', 'model_provider': 'ollama'}, id='lc_run--019b8922-0055-7363-861c-caebc8a0e5bb-0', usage_metadata={'input_tokens': 555, 'output_tokens': 35, 'total_tokens': 590})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba12edc",
   "metadata": {},
   "source": [
    "### Streaming with astream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d61ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] \n",
    "async for token in ollama_llm.astream(\"What are autoencoders?\"):\n",
    "    tokens.append(token)\n",
    "    print (token.content, end='|', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875b563",
   "metadata": {},
   "source": [
    "### Implementing streaming feature for agent. \n",
    "\n",
    "As we know to build an agent executor we need - \n",
    "- Tools \n",
    "- ChatPromptTemplate\n",
    "- LLM \n",
    "- Agent \n",
    "- Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fac1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6283245",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add(x:float, y:float) -> float: \n",
    "    \"Add 'x' and 'y' \"\n",
    "    return x + y \n",
    "\n",
    "@tool\n",
    "def substract(x:float, y:float) -> float: \n",
    "    \"Substract 'y' from 'x' \"\n",
    "    return x - y \n",
    "\n",
    "@tool\n",
    "def multiply(x:float, y:float) -> float: \n",
    "    \"Multiply 'x' and 'y' \"\n",
    "    return x * y \n",
    "\n",
    "@tool\n",
    "def divide(x:float, y:float) -> float: \n",
    "    \"Divide 'x' by 'y' \"\n",
    "    return x / y \n",
    "\n",
    "@tool\n",
    "def exponential(x:float, y:float) -> float: \n",
    "    \"Raise 'x' to the power of 'y' \"\n",
    "    return x ** y\n",
    "\n",
    "@tool \n",
    "def final_answer(answer:str, tools_used:list[str]) -> str:\n",
    "    \"\"\" Use this tool to provide the final answer to the user. \n",
    "    The answer should be in natural language as this will be provided to user directly.\n",
    "    tools_used should contain list of tools used within the `scratchpad`\n",
    "    \"\"\"\n",
    "\n",
    "    return {'answer': answer, \"tools_used\": tools_used}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83133be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of tools \n",
    "tools = [add, substract, multiply, divide, exponential, final_answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b047d266",
   "metadata": {},
   "source": [
    "### Chat Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434bc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful assistant. When answering user's query you should first use one of the tool provided. \"\n",
    "        \"After using the tool, the tool output will be provided in scratchpad below.\"\n",
    "        \"You MUST then use final_answer tool to provide an answer to the user. DO NOT use the same tool more than once.\"\n",
    "    )),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e2a3cf",
   "metadata": {},
   "source": [
    "### LCEL Declaration for agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8a5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import RunnableSerializable\n",
    "\n",
    "agent : RunnableSerializable = (\n",
    "    {\n",
    "        \"input\" : lambda x : x[\"input\"], \n",
    "        \"chat_history\" : lambda x : x[\"chat_history\"], \n",
    "        \"agent_scratchpad\" : lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt \n",
    "    | ollama_llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42a296",
   "metadata": {},
   "source": [
    "### Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1acdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# create name2tool lookup \n",
    "name2tool = {tool.name : tool.func for tool in tools}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c765d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': <function __main__.add(x: float, y: float) -> float>,\n",
       " 'substract': <function __main__.substract(x: float, y: float) -> float>,\n",
       " 'multiply': <function __main__.multiply(x: float, y: float) -> float>,\n",
       " 'divide': <function __main__.divide(x: float, y: float) -> float>,\n",
       " 'exponential': <function __main__.exponential(x: float, y: float) -> float>,\n",
       " 'final_answer': <function __main__.final_answer(answer: str, tools_used: list[str]) -> str>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2871e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama_openai_llm = ChatOpenAI(temperature=0, model=\"ministral-3:14b\", api_key=\"ollama_key\", base_url=os.getenv('OLLAMA_BASE_URL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50725e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgentExecutor: \n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iteration:int=3):\n",
    "        self.chat_history = [] \n",
    "        self.max_iterations = max_iteration\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\" : lambda x : x[\"input\"], \n",
    "                \"chat_history\" : lambda x : x[\"chat_history\"], \n",
    "                \"agent_scratchpad\" : lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt \n",
    "            | openai_llm.bind_tools(tools, tool_choice=\"any\")\n",
    "        )\n",
    "\n",
    "    def invoke(self, input:str) -> dict:\n",
    "        count = 0\n",
    "        agent_scratchpad = [] \n",
    "\n",
    "        while count < self.max_iterations:\n",
    "            out = self.agent.invoke({\n",
    "                \"input\": input, \n",
    "                \"chat_history\": self.chat_history, \n",
    "                \"agent_scratchpad\": agent_scratchpad\n",
    "            })\n",
    "\n",
    "            # if final_answer tool called -> come out of the loop \n",
    "            print (out)\n",
    "            print()\n",
    "            print(out.tool_calls)\n",
    "\n",
    "            # if out.tool_calls:\n",
    "\n",
    "            if out.tool_calls[0][\"name\"] == \"final_answer\":\n",
    "                break \n",
    "            \n",
    "            # add the output to agent scratchpad\n",
    "            agent_scratchpad.append(out)\n",
    "            tool_out = name2tool[out.tool_calls[0][\"name\"]](**out.tool_calls[0][\"args\"])\n",
    "            action_str = f\"The {out.tool_calls[0]['name']} tool returned {tool_out}\"\n",
    "            agent_scratchpad.append({\n",
    "                \"role\": \"tool\", \n",
    "                \"content\": action_str, \n",
    "                \"tool_call_id\": out.tool_calls[0][\"id\"]\n",
    "            })\n",
    "\n",
    "            print (f\"{count}: {action_str}\")\n",
    "            count += 1 \n",
    "\n",
    "        final_answer = out.tool_calls[0][\"args\"]\n",
    "        final_answer_str = json.dumps(final_answer)\n",
    "        self.chat_history.append({\n",
    "            \"input\": input, \n",
    "            \"output\": final_answer_str\n",
    "        })\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input), \n",
    "            AIMessage(content=final_answer_str)\n",
    "        ])\n",
    "\n",
    "        return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "126ec63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_agent = CustomAgentExecutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6c36b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 281, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuIFJ0QeVsDxmBpPiWMyZQ77vflJ8', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b8926-3171-77f0-92be-3a038947ed44-0' tool_calls=[{'name': 'add', 'args': {'x': 2, 'y': 8}, 'id': 'call_GeIWGWQ4K8kWG6vazazWz7Td', 'type': 'tool_call'}] usage_metadata={'input_tokens': 281, 'output_tokens': 17, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "[{'name': 'add', 'args': {'x': 2, 'y': 8}, 'id': 'call_GeIWGWQ4K8kWG6vazazWz7Td', 'type': 'tool_call'}]\n",
      "0: The add tool returned 10\n",
      "content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 311, 'total_tokens': 342, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuIFK1QhKjqzXMNwCLt8XEKYa4LiN', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b8926-383f-7873-831e-b71305785c0b-0' tool_calls=[{'name': 'final_answer', 'args': {'answer': 'The result of 2 + 8 is 10.', 'tools_used': ['functions.add']}, 'id': 'call_ronD3syKPnKw2njJzAhvO8kV', 'type': 'tool_call'}] usage_metadata={'input_tokens': 311, 'output_tokens': 31, 'total_tokens': 342, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "[{'name': 'final_answer', 'args': {'answer': 'The result of 2 + 8 is 10.', 'tools_used': ['functions.add']}, 'id': 'call_ronD3syKPnKw2njJzAhvO8kV', 'type': 'tool_call'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'The result of 2 + 8 is 10.', 'tools_used': ['functions.add']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_agent.invoke(input=\"What is 2+8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c76f1b",
   "metadata": {},
   "source": [
    "```\n",
    "Works well for single operation. Fails with key error for role and all for multiple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58579eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': 'What is 2+8',\n",
       "  'output': '{\"answer\": \"The result of 2 + 8 is 10.\", \"tools_used\": [\"functions.add\"]}'},\n",
       " HumanMessage(content='What is 2+8', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='{\"answer\": \"The result of 2 + 8 is 10.\", \"tools_used\": [\"functions.add\"]}', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_agent.chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bd387",
   "metadata": {},
   "source": [
    "``` \n",
    "Notice ollama is putting everying in content instead of tool call :) \n",
    "Need to fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00e771f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='add[ARGS]{\"x\": 2, \"y\": 8}' additional_kwargs={} response_metadata={'model': 'ministral-3:14b', 'created_at': '2026-01-04T13:17:01.772463Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11086397542, 'load_duration': 126107708, 'prompt_eval_count': 455, 'prompt_eval_duration': 9906908583, 'eval_count': 15, 'eval_duration': 1005357207, 'logprobs': None, 'model_name': 'ministral-3:14b', 'model_provider': 'ollama'} id='lc_run--019b8927-447a-7e81-8834-ab04a199f5d9-0' usage_metadata={'input_tokens': 455, 'output_tokens': 15, 'total_tokens': 470}\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m custom_agent_ministral = CustomAgentExecutor()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcustom_agent_ministral\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is 2+8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mCustomAgentExecutor.invoke\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(out.tool_calls)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# if out.tool_calls:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mfinal_answer\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m \n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# add the output to agent scratchpad\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "custom_agent_ministral = CustomAgentExecutor()\n",
    "custom_agent_ministral.invoke(input=\"What is 2+8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32209bcf",
   "metadata": {},
   "source": [
    "### Implementing Async call to Agent Executor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8364474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define aysncio llm for streaming the data \n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "stream_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", streaming=True).configurable_fields(callbacks=ConfigurableField(id=\"callbacks\", name=\"callbacks\", description=\"List of callbacks to use for streaming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "809b89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent : RunnableSerializable = (\n",
    "    {\n",
    "        \"input\" : lambda x : x[\"input\"], \n",
    "        \"chat_history\" : lambda x : x[\"chat_history\"], \n",
    "        \"agent_scratchpad\" : lambda x: x.get(\"agent_scratchpad\", [])\n",
    "    }\n",
    "    | prompt \n",
    "    | stream_llm.bind_tools(tools, tool_choice=\"any\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f3d20135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "from langchain_core.callbacks.base import AsyncCallbackHandler\n",
    "\n",
    "class QueueCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\" Call back handler to put tokens generated to queue \"\"\"\n",
    "\n",
    "    def __init__(self, queue:asyncio.Queue):\n",
    "        self.queue = queue \n",
    "        self.final_answer_seen = False \n",
    "\n",
    "    async def __aiter__(self):\n",
    "        while True: \n",
    "            if self.queue.empty():\n",
    "                await asyncio.sleep(0.1)\n",
    "                continue \n",
    "\n",
    "            token_or_done = await self.queue.get()\n",
    "\n",
    "            if token_or_done == \"<<DONE>>\":\n",
    "                return \n",
    "            \n",
    "            if token_or_done:\n",
    "                yield token_or_done\n",
    "\n",
    "    async def on_llm_new_token(self, *args, **kwargs) -> None:\n",
    "        \"\"\" Put token on the queue \"\"\"\n",
    "\n",
    "        chunk = kwargs.get(\"chunk\")\n",
    "        if chunk: \n",
    "\n",
    "            # := operator will assign the value in the right side to left side; it is like checking if left has something or not and equal to right side\n",
    "            if tool_calls := chunk.message.additional_kwargs.get(\"tool_calls\"):\n",
    "                if tool_calls[0][\"function\"][\"name\"] == \"final_answer\":\n",
    "                    self.final_answer_seen = True \n",
    "\n",
    "        self.queue.put_nowait(kwargs.get(\"chunk\"))\n",
    "        return \n",
    "    \n",
    "\n",
    "    async def on_llm_end(self, *args, **kwargs) -> None:\n",
    "        \"\"\" Put message to queue to signal completion\"\"\"\n",
    "\n",
    "        if self.final_answer_seen:\n",
    "            self.queue.put_nowait(\"<<DONE>>\")\n",
    "        else: \n",
    "            self.queue.put_nowait(\"<<STEP_END>>\")\n",
    "        return \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "afe4456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1e2e14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] \n",
    "async def stream(query:str):\n",
    "    response = agent.with_config(callbacks=[streamer])\n",
    "\n",
    "    async for token in response.astream({\n",
    "        \"input\" : query, \n",
    "        \"chat_history\": [], \n",
    "        \"agent_scratchpad\": []\n",
    "    }):\n",
    "        tokens.append(token)\n",
    "        print(token, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4dc27028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' tool_calls=[{'name': 'add', 'args': {}, 'id': 'call_0xxQ3mrbi0s6qtyeUwmY0G3N', 'type': 'tool_call'}] tool_call_chunks=[{'name': 'add', 'args': '', 'id': 'call_0xxQ3mrbi0s6qtyeUwmY0G3N', 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}] tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': 'x', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'x', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': '788', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '788', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': ',\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': ',\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': 'y', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': 'y', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': '25', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '25', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}] tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}]\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'service_tier': 'default', 'model_provider': 'openai'} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18'\n",
      "content='' additional_kwargs={} response_metadata={} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' usage_metadata={'input_tokens': 282, 'output_tokens': 17, 'total_tokens': 299, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='' additional_kwargs={} response_metadata={} id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18' chunk_position='last'\n"
     ]
    }
   ],
   "source": [
    "await stream(\"What is 788+25?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "645ecb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai', 'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'service_tier': 'default'}, id='lc_run--019b897a-c319-7352-a4f5-a771a5836f18', tool_calls=[{'name': 'add', 'args': {'x': 788, 'y': 25}, 'id': 'call_0xxQ3mrbi0s6qtyeUwmY0G3N', 'type': 'tool_call'}], usage_metadata={'input_tokens': 282, 'output_tokens': 17, 'total_tokens': 299, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, tool_call_chunks=[{'name': 'add', 'args': '{\"x\":788,\"y\":25}', 'id': 'call_0xxQ3mrbi0s6qtyeUwmY0G3N', 'index': 0, 'type': 'tool_call_chunk'}], chunk_position='last')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see one single message from all these streams values \n",
    "tk = tokens[0]\n",
    "for token in tokens[1:]: \n",
    "    tk += token\n",
    "tk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980d3ab",
   "metadata": {},
   "source": [
    "### Modify our CustomAgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed225b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class CustomStreamAgentExecutor:\n",
    "    chat_history = list[BaseMessage]\n",
    "\n",
    "    def __init__(self, max_iterations:int=3):\n",
    "        self.chat_history = [] \n",
    "        self.max_iterations = max_iterations\n",
    "        self.agent: RunnableSerializable = (\n",
    "            {\n",
    "                \"input\": lambda x: x[\"input\"], \n",
    "                \"chat_history\": lambda x: x[\"chat_history\"], \n",
    "                \"agent_scratchpad\": lambda x: x.get(\"agent_scratchpad\", [])\n",
    "            }\n",
    "            | prompt \n",
    "            | stream_llm.bind_tools(tools, tool_choice=\"any\")\n",
    "        )\n",
    "\n",
    "    async def invoke(self, input:str, streamer:QueueCallbackHandler, verbose:bool=False):\n",
    "        count = 0 \n",
    "        agent_scratchpad = [] \n",
    "\n",
    "        while count < self.max_iterations:\n",
    "            async def stream(query:str):\n",
    "                response = self.agent.with_config(callbacks=[streamer])\n",
    "                output = None \n",
    "                async for token in response.astream({\n",
    "                    \"input\": query, \n",
    "                    \"chat_history\": self.chat_history, \n",
    "                    \"agent_scratchpad\": agent_scratchpad\n",
    "                }):\n",
    "                    if output is None: \n",
    "                        output = token \n",
    "                    else:\n",
    "                        output += token \n",
    "                    \n",
    "                    if token.content != \"\": \n",
    "                        if verbose: print (f\"content: {token.content}\", flush=True)\n",
    "\n",
    "\n",
    "                    tool_calls = token.additional_kwargs.get(\"tool_calls\")\n",
    "                    if tool_calls:\n",
    "                        if verbose: print(f\"tools call: {tool_calls}\", flush=True)\n",
    "                        tool_name = tool_calls[0][\"function\"][\"name\"]\n",
    "\n",
    "                        if tool_name:\n",
    "                            if verbose: print (f\"tool name: {tool_name}\", flush=True)\n",
    "                        \n",
    "                            arg = tool_calls[0][\"function\"][\"arguments\"]\n",
    "                            if arg != \"\": \n",
    "                                if verbose: print (f\"Arguments: {arg}\", flush=True)\n",
    "\n",
    "                return AIMessage(\n",
    "                    content=output.content, \n",
    "                    tool_calls=output.tool_calls, \n",
    "                    tool_call_id=output.tool_calls[0][\"id\"]\n",
    "                )\n",
    "            \n",
    "            tool_call = await stream(query=input)\n",
    "            agent_scratchpad.append(tool_call)\n",
    "            tool_name = tool_call.tool_calls[0][\"name\"]\n",
    "            tool_args = tool_call.tool_calls[0][\"args\"]\n",
    "            tool_call_id = tool_call.tool_call_id\n",
    "\n",
    "            tool_out = name2tool[tool_name](**tool_args)\n",
    "            tool_exec = ToolMessage(\n",
    "                content=f\"{tool_out}\", \n",
    "                tool_call_id=tool_call_id\n",
    "            )\n",
    "\n",
    "            agent_scratchpad.append(tool_exec)\n",
    "            count += 1\n",
    "\n",
    "            if tool_name == \"final_answer\":\n",
    "                break \n",
    "        \n",
    "        final_answer = tool_out[\"answer\"]\n",
    "        self.chat_history.extend([\n",
    "            HumanMessage(content=input), \n",
    "            AIMessage(content=final_answer)\n",
    "        ])\n",
    "\n",
    "        return tool_args\n",
    "    \n",
    "\n",
    "custom_stream_agent = CustomStreamAgentExecutor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "75a31aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue=asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "out = await custom_stream_agent.invoke(\"What is 788+25?\", streamer=streamer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "35bf4ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The sum of 788 and 25 is 813.', 'tools_used': ['functions.add']}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba46a48",
   "metadata": {},
   "source": [
    "### Right way to print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "388e3b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', tool_calls=[{'name': 'add', 'args': {}, 'id': 'call_nvPpk83yfdoDjgu6id2db4z5', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'add', 'args': '', 'id': 'call_nvPpk83yfdoDjgu6id2db4z5', 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}], tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': 'x', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'x', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': '788', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '788', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': ',\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ',\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': 'y', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'y', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': '\":', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': '25', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '25', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "generation_info={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'service_tier': 'default'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5')\n",
      "generation_info={} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', usage_metadata={'input_tokens': 309, 'output_tokens': 17, 'total_tokens': 326, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--019b897b-23b9-7243-9326-0d8fcaa3d4e5', chunk_position='last')\n",
      "<<STEP_END>>\n",
      "\n",
      "\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', tool_calls=[{'name': 'final_answer', 'args': {}, 'id': 'call_O7ie2mPnwIOE7VpY1hEb23sD', 'type': 'tool_call'}], tool_call_chunks=[{'name': 'final_answer', 'args': '', 'id': 'call_O7ie2mPnwIOE7VpY1hEb23sD', 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', tool_calls=[{'name': '', 'args': {}, 'id': None, 'type': 'tool_call'}], tool_call_chunks=[{'name': None, 'args': '{\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': 'answer', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'answer', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '\":\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': 'The', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'The', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' sum', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' sum', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' of', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' of', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' ', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '788', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '788', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' and', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' and', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' ', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '25', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '25', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' is', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' is', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': ' ', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': ' ', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '813', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '813', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '.\",\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.\",\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': 'tools', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'tools', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '_used', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '_used', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '\":[\"', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\":[\"', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': 'functions', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': 'functions', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '.add', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '.add', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '\"]', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '\"]', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', invalid_tool_calls=[{'name': None, 'args': '}', 'id': None, 'error': None, 'type': 'invalid_tool_call'}], tool_call_chunks=[{'name': None, 'args': '}', 'id': None, 'index': 0, 'type': 'tool_call_chunk'}])\n",
      "generation_info={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'service_tier': 'default'} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb')\n",
      "generation_info={} message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', usage_metadata={'input_tokens': 334, 'output_tokens': 31, 'total_tokens': 365, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "message=AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--019b897b-279e-7cf0-9704-fe51a6e9b2bb', chunk_position='last')\n",
      "<<STEP_END>>\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m streamer = QueueCallbackHandler(queue)\n\u001b[32m      3\u001b[39m task = asyncio.create_task(custom_stream_agent.invoke(\u001b[33m\"\u001b[39m\u001b[33mWhat is 788+25?\u001b[39m\u001b[33m\"\u001b[39m, streamer))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m streamer:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m (token)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token == \u001b[33m\"\u001b[39m\u001b[33m<<STEP_END>>\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mQueueCallbackHandler.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.queue.empty():\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m \n\u001b[32m     17\u001b[39m     token_or_done = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.queue.get()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/asyncio/tasks.py:665\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    661\u001b[39m h = loop.call_later(delay,\n\u001b[32m    662\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    663\u001b[39m                     future, result)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    667\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "queue=asyncio.Queue()\n",
    "streamer = QueueCallbackHandler(queue)\n",
    "task = asyncio.create_task(custom_stream_agent.invoke(\"What is 788+25?\", streamer))\n",
    "\n",
    "async for token in streamer:\n",
    "    print (token)\n",
    "    if token == \"<<STEP_END>>\":\n",
    "        print (\"\\n\", flush=True)\n",
    "    \n",
    "    elif tool_calls := token.message.additional_kwargs.get(\"tool_calls\"):\n",
    "        if tool_name := tool_calls[0][\"function\"][\"name\"]:\n",
    "            print (f\"Calling {tool_name}\", flush=True)\n",
    "        if tool_args := tool_calls[0][\"function\"][\"arguments\"]:\n",
    "            print (f\"({tool_args})\", end=\"\", flush=True)\n",
    "\n",
    "_ = await task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7213d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generativeai-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
