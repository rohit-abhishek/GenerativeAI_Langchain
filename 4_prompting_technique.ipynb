{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c4ec9d3",
   "metadata": {},
   "source": [
    "# prompting technique \n",
    "\n",
    "Take a latest news from newspaper which LLM will not have answer to e.g. \n",
    "on 27 Dec 2025 - Taiwan is rattled by earthquake of magniture 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83ac1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33aa2ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# create llms \n",
    "google_llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-3-flash-preview\")\n",
    "openai_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8ad9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets give some context to the LLM for question which will be asked later\n",
    "context = \"\"\" \n",
    "A massive earthquake of magnitude 7.0 on the Richter Scale struck Taiwan on Saturday 27 Dec 2025, \n",
    "according to the country's weather agency, reported news agency AFP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1b0028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create system and human prompts \n",
    "# from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# create system prompt with and without context \n",
    "system_prompt_without_context = \"\"\" \n",
    "You are helpful assistant that helps people find information. You are given a question and you need to provide detailed and accurate answer.\n",
    "Answer with \"I don't know\" if you are not absolutely sure.\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_with_context = \"\"\" \n",
    "You are helpful assistant that helps people find information. You are given a question and you need to provide detailed and accurate answer.\n",
    "Use context to answer: \n",
    "{context}\n",
    "\n",
    "Answer with I don't know if you are not absolutely sure. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d090815",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\" What was the magnitude of the earthquake recorded in Taiwan on 27 Dec 2025?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f38f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the prompt for llm \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template_without_context = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_without_context),\n",
    "    (\"user\", \"{user_prompt}\")\n",
    "])\n",
    "\n",
    "\n",
    "prompt_template_with_context = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_with_context),\n",
    "    (\"user\", \"{user_prompt}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1371b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_prompt']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_without_context.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "702e9fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['context', 'user_prompt']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_with_context.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85104e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ChatPromptTemplate(input_variables=['user_prompt'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=' \\nYou are helpful assistant that helps people find information. You are given a question and you need to provide detailed and accurate answer.\\nAnswer with \"I don\\'t know\" if you are not absolutely sure.\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_prompt'], input_types={}, partial_variables={}, template='{user_prompt}'), additional_kwargs={})]),\n",
       " ChatPromptTemplate(input_variables=['context', 'user_prompt'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\" \\nYou are helpful assistant that helps people find information. You are given a question and you need to provide detailed and accurate answer.\\nUse context to answer: \\n{context}\\n\\nAnswer with I don't know if you are not absolutely sure. \\n\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_prompt'], input_types={}, partial_variables={}, template='{user_prompt}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_without_context, \\\n",
    "prompt_template_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22b72ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First call without context to see if LLM knows \n",
    "pipeline_without_context = (\n",
    "    {\"user_prompt\": lambda x : x[\"user_prompt\"]}\n",
    "    | prompt_template_without_context\n",
    "    | google_llm\n",
    ")\n",
    "\n",
    "answer = pipeline_without_context.invoke({\"user_prompt\" : user_prompt})\n",
    "answer.content[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "701d93d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The magnitude of the earthquake recorded in Taiwan on 27 Dec 2025 was 7.0 on the Richter Scale.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First call without context to see if LLM knows \n",
    "pipeline_with_context = (\n",
    "    {\n",
    "        \"user_prompt\": lambda x : x[\"user_prompt\"], \n",
    "        \"context\" : lambda x : x[\"context\"]\n",
    "    }\n",
    "    | prompt_template_with_context\n",
    "    | google_llm\n",
    ")\n",
    "\n",
    "answer = pipeline_with_context.invoke({\"user_prompt\" : user_prompt, \"context\": context})\n",
    "answer.content[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215d4d7",
   "metadata": {},
   "source": [
    "# Few shot prompting \n",
    "Not needed in Frontier model now. However for local LLM we can provide some of the examples, using these examples LLM has to respond in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b80f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"), \n",
    "    (\"ai\", \"{output}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bdbd7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"here is query #1\", \"output\": \"# Here is answer for #1\"},\n",
    "    {\"input\": \"here is query #2\", \"output\": \"# Here is answer for #2\"},\n",
    "    {\"input\": \"here is query #3\", \"output\": \"# Here is answer for #3\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dcaf6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "fewshot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1afe64a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: here is query #1\n",
      "AI: # Here is answer for #1\n",
      "Human: here is query #2\n",
      "AI: # Here is answer for #2\n",
      "Human: here is query #3\n",
      "AI: # Here is answer for #3\n"
     ]
    }
   ],
   "source": [
    "print(fewshot_prompt.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b405c84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_prompt = \"\"\"\n",
    "You are helpful assistant that helps people find information. You are given a question and you need to provide detailed and accurate answer.\n",
    "Always answer in markdown format\n",
    "Use context to answer: \n",
    "{context}\n",
    "\n",
    "Answer with I don't know if you are not absolutely sure. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "104cb61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nYou are helpful assistant that helps people find information. You are given a question and you need to provide detailed and accurate answer.\\nUse context to answer: \\n{context}\\n\\nAnswer with I don't know if you are not absolutely sure. \\n\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_with_context.messages[0].prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f01acb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", new_system_prompt),\n",
    "    fewshot_prompt, \n",
    "    (\"user\", \"{user_prompt}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c9834e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshot_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8edb3675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The magnitude of the earthquake recorded in Taiwan on 27 Dec 2025 was **7.0** on the Richter Scale.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First call without context to see if LLM knows \n",
    "pipeline_with_context = (\n",
    "    {\n",
    "        \"user_prompt\": lambda x : x[\"user_prompt\"], \n",
    "        \"context\" : lambda x : x[\"context\"]\n",
    "    }\n",
    "    | prompt_template\n",
    "    | google_llm\n",
    ")\n",
    "\n",
    "answer = pipeline_with_context.invoke({\"user_prompt\" : user_prompt, \"context\": context})\n",
    "answer.content[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347ad85",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting \n",
    "More useful where we want LLM to think before responding to the question asked. \n",
    "Frontier models these day typically use chain of thought by default but for smaller LLMs this can be really useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ab7d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this get the ollama 3.1 model \n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "ollama_llm = OllamaLLM(temperature=0.0, model=\"gemma3:12b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b895eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no chain of thoughts system prompt \n",
    "no_cots_system_prompt = \"\"\"\n",
    "You are a helpful assistant and answer questions asked by the user. \n",
    "\n",
    "You MUST answer questions directly without any other text or explanation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c8b604d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cots_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", no_cots_system_prompt), \n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "630b4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\" How many keystrokes are needed to type numbers between 1 to 500\"\"\"\n",
    "\n",
    "no_cots_pipeline = no_cots_prompt_template | ollama_llm\n",
    "no_cots_result = no_cots_pipeline.invoke({\"query\" : query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4666db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1120'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_cots_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e74636",
   "metadata": {},
   "source": [
    "``` \n",
    "the actual answer is 1392 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36010805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use chain of thoughts \n",
    "cots_system_prompt = \"\"\"\n",
    "You are a helpful assistant and answer questions asked by the user. \n",
    "To answer the question, you must: \n",
    "- List systematically and precisely details of all subproblem that needs to be solved to answer the question \n",
    "- Solve each subproblem INDIVIDUALLY and in sequence \n",
    "- Finally, use everything you have worked through to provide the final answer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b0ae850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cots_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", cots_system_prompt), \n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6657dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cots_pipeline = cots_prompt_template | ollama_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "29191d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cots_result = cots_pipeline.invoke({\"query\" : query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ff4e4f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break this down systematically. We need to calculate the total number of keystrokes required to type the numbers from 1 to 500.\n",
       "\n",
       "**Subproblem 1: Keystrokes for 1-digit numbers (1-9)**\n",
       "\n",
       "*   Each 1-digit number requires 1 keystroke.\n",
       "*   There are 9 such numbers (1 to 9).\n",
       "*   Total keystrokes: 9 * 1 = 9\n",
       "\n",
       "**Subproblem 2: Keystrokes for 2-digit numbers (10-99)**\n",
       "\n",
       "*   Each 2-digit number requires 2 keystrokes.\n",
       "*   There are 99 - 10 + 1 = 90 such numbers.\n",
       "*   Total keystrokes: 90 * 2 = 180\n",
       "\n",
       "**Subproblem 3: Keystrokes for 3-digit numbers (100-499)**\n",
       "\n",
       "*   Each 3-digit number requires 3 keystrokes.\n",
       "*   There are 499 - 100 + 1 = 400 such numbers.\n",
       "*   Total keystrokes: 400 * 3 = 1200\n",
       "\n",
       "**Subproblem 4: Keystrokes for the number 500**\n",
       "\n",
       "*   The number 500 requires 3 keystrokes.\n",
       "\n",
       "**Subproblem 5: Total Keystrokes**\n",
       "\n",
       "*   Add the keystrokes from all the subproblems.\n",
       "\n",
       "**Solving the Subproblems:**\n",
       "\n",
       "*   **Subproblem 1:** 9 keystrokes\n",
       "*   **Subproblem 2:** 180 keystrokes\n",
       "*   **Subproblem 3:** 1200 keystrokes\n",
       "*   **Subproblem 4:** 3 keystrokes\n",
       "\n",
       "**Final Answer:**\n",
       "\n",
       "Total keystrokes = 9 + 180 + 1200 + 3 = 1392\n",
       "\n",
       "Therefore, 1392 keystrokes are needed to type the numbers from 1 to 500."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.Markdown(cots_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15709572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generativeai-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
