{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15016b43",
   "metadata": {},
   "source": [
    "# API calls are REST based and stateless. \n",
    "Everytime call is made to Model - it is brand new request for LLM. So, you can provide some memory to LLM with various technique to keep track of chat conversation which has already happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9dae46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7955b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set langsmit tracing off \n",
    "os.environ['LANGSMITH_TRACING'] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f240a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# create llms \n",
    "google_llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-3-flash-preview\")\n",
    "openai_llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "ollama_llm = ChatOllama(temperature=0, model=\"gemma3:12b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632c802",
   "metadata": {},
   "source": [
    "### Chat memory \n",
    "Conversation memory can be maintained using any of 4 techniques:\n",
    "- ConversationBufferedMemory: <br>\n",
    "Keep history of all interactions in a list. When making a call to LLM use this history list as context for LLM to keep track of what was discussed before. \n",
    "\n",
    "- ConversationBufferedWindowMemory: <br>\n",
    "Keep track of all interactions in a list, but when making a call to LLM use only latest N conversations as context to LLM \n",
    "\n",
    "- ConversationSummaryMemory: <br> \n",
    "Instead of all conversations, just summarize and use this as input to LLM \n",
    "\n",
    "- ConversationSummaryBufferedMemory: <br>\n",
    "Use Summary + Latest N interactions as context to LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a68d29",
   "metadata": {},
   "source": [
    "### 1. ConversationBufferedMemory using RunnableWithMessageHistory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75cf2c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant and your name is Gamma. \n",
    "You provide answers to questions to best of your knowledge. \n",
    "If you dont know just say \"I don't know\" \"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57cefc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['history', 'query']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ace0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['history', 'query'], input_types={'history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10e8e3a60>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant and your name is Gamma. \\nYou provide answers to questions to best of your knowledge. \\nIf you dont know just say \"I don\\'t know\" '), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c0ec92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the pipeline \n",
    "pipeline = prompt_template | ollama_llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fcbe7",
   "metadata": {},
   "source": [
    "```\n",
    "To have this running we want to have RunnableWithMessageHistory wrapped around our pipeline. The object RunnableWithMessageHistory requires input parameters. One of those is get_session_history which returns ChatMessageHistory object based on a session id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "768649d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "chat_map = {} \n",
    "\n",
    "def get_chat_history(session_id) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "652683c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline, \n",
    "    get_session_history=get_chat_history, \n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0132545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit! It's nice to meet you. I'm Gamma, your helpful assistant. What can I do for you today?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:39:10.784067Z', 'done': True, 'done_reason': 'stop', 'total_duration': 24662831250, 'load_duration': 5656928667, 'prompt_eval_count': 59, 'prompt_eval_duration': 16923610084, 'eval_count': 29, 'eval_duration': 2012652712, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb2-7966-7652-9baa-59c72c1ed9b9-0', usage_metadata={'input_tokens': 59, 'output_tokens': 29, 'total_tokens': 88})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke({\"query\": \"Hi my name is Rohit\"}, \n",
    "                             config={\"session_id\": \"id_123\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c897e2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Rohit! Absolutely, I can help with that.\\n\\nAutoencoders are a type of neural network used for unsupervised learning. They learn to compress and then reconstruct input data. Think of it as a sophisticated compression algorithm! \\n\\nEssentially, an autoencoder has an encoder (compresses) and a decoder (reconstructs). You train it by feeding it data and minimizing the difference between the original and reconstructed data. They're used for dimensionality reduction, feature extraction, anomaly detection, and even generative tasks.\\n\\n\\n\\nHope this helps with your research!\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:39:29.379343Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11933545583, 'load_duration': 302912041, 'prompt_eval_count': 126, 'prompt_eval_duration': 3638545542, 'eval_count': 112, 'eval_duration': 7926863742, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb2-f3b8-7b43-ac1c-5df521221f59-0', usage_metadata={'input_tokens': 126, 'output_tokens': 112, 'total_tokens': 238})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke({\"query\": \"I am working on research paper for Autoencoders. Can you help me in 100 words what is it and how to use it?\"}, \n",
    "                             config={\"session_id\": \"id_123\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47b92629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Rohit! ðŸ˜Š', additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:39:30.313201Z', 'done': True, 'done_reason': 'stop', 'total_duration': 916135417, 'load_duration': 191199292, 'prompt_eval_count': 253, 'prompt_eval_duration': 283362875, 'eval_count': 7, 'eval_duration': 430488667, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb3-2273-7bf3-bd7c-39738fb2e525-0', usage_metadata={'input_tokens': 253, 'output_tokens': 7, 'total_tokens': 260})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke({\"query\": \"What is my name again?\"}, \n",
    "                             config={\"session_id\": \"id_123\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24630fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. I am Gamma, a helpful assistant, and I don't have access to personal information like that. ðŸ˜Š \\n\\n\\n\\nDo you want to tell me your name?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:39:33.708557Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3380349375, 'load_duration': 167024667, 'prompt_eval_count': 59, 'prompt_eval_duration': 272984375, 'eval_count': 42, 'eval_duration': 2917967586, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb3-2617-78f2-83cb-52db2ea04f2b-0', usage_metadata={'input_tokens': 59, 'output_tokens': 42, 'total_tokens': 101})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets change the session id and see if any of these work \n",
    "pipeline_with_history.invoke({\"query\": \"What is my name?\"}, \n",
    "                             config={\"session_id\": \"id_567\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33f5ec9",
   "metadata": {},
   "source": [
    "### 2. ConversationBufferWindowMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9399b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field \n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferedWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k:int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k:int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Instantiating BufferWindowMessageHistory with k = {k}\")\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]):\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self):\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e377bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {} \n",
    "\n",
    "def get_chat_history(session_id:str, k:int=4) -> BufferedWindowMessageHistory:\n",
    "    print(f\"get_chat_history called with session={session_id} and k={k}\")\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferedWindowMessageHistory(k=k)\n",
    "    return chat_map[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30631b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets wrap the buffered window message history inside runnablewithmessagehistory object \n",
    "from langchain_core.runnables import ConfigurableFieldSpec \n",
    "pipeline_with_trimmed_history = RunnableWithMessageHistory(\n",
    "    pipeline, \n",
    "    get_session_history=get_chat_history, \n",
    "    input_messages_key=\"query\", \n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\", \n",
    "            annotation=str, \n",
    "            name=\"Session ID\", \n",
    "            description=\"Session id to use for Chat History\", \n",
    "            default=\"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\", \n",
    "            annotation=int, \n",
    "            name=\"k\",\n",
    "            description=\"Number of messages to keep in history\", \n",
    "            default=4\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb5364de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n",
      "Instantiating BufferWindowMessageHistory with k = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit! It's nice to meet you. I'm Gamma, your helpful assistant. What can I do for you today?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:39:45.179455Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2701295833, 'load_duration': 208145875, 'prompt_eval_count': 59, 'prompt_eval_duration': 489047875, 'eval_count': 29, 'eval_duration': 1988183705, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb3-558c-7312-b0b3-038bfda1e377-0', usage_metadata={'input_tokens': 59, 'output_tokens': 29, 'total_tokens': 88})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"Hi my name is Rohit\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "684a9eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Absolutely, Rohit! Here's a brief explanation of autoencoders:\\n\\nAutoencoders are a type of neural network used for unsupervised learning. They aim to learn a compressed, efficient representation (encoding) of input data. They do this by training a network to reconstruct its input. Essentially, they have an encoder that reduces dimensionality and a decoder that expands it back. They're used for dimensionality reduction, feature extraction, and anomaly detection.\\n\\n\\n\\nHope that helps!\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:39:54.355927Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7635918291, 'load_duration': 205265041, 'prompt_eval_count': 121, 'prompt_eval_duration': 703866250, 'eval_count': 94, 'eval_duration': 6667691883, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb3-661e-7d52-8b64-d93545378393-0', usage_metadata={'input_tokens': 121, 'output_tokens': 94, 'total_tokens': 215})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"I am working on autoencoders. Can you tell me briefly about autoencoders in less than 100 words?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "757b7f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, Rohit, that\\'s a challenging but interesting situation! Dealing with 1000+ categorical columns with minimal preprocessing requires a strategic approach. Here\\'s a breakdown of potential methods, considering their pros and cons, to understand relationships without heavy preprocessing:\\n\\n**1. Autoencoders (Your Existing Interest - Potentially Powerful)**\\n\\n*   **How it works:** You can adapt autoencoders to handle categorical data directly. Instead of one-hot encoding (which would explode the dimensionality), you could use embedding layers for each categorical feature. The autoencoder learns to reconstruct the input, and the \"latent space\" (the compressed representation) reveals relationships.\\n*   **Pros:** Can capture complex, non-linear relationships. Doesn\\'t require explicit feature engineering.\\n*   **Cons:** Can be computationally expensive to train. Requires careful tuning of architecture and hyperparameters. Interpretation of the latent space can be tricky.  You\\'ll need to experiment with different embedding sizes.\\n\\n**2. Correlation-Based Methods (with Modifications)**\\n\\n*   **How it works:** Standard correlation coefficients (like Pearson) aren\\'t ideal for categorical data. However, you can use methods like Cramer\\'s V or Chi-squared tests to measure association between categorical variables.  You\\'d calculate these pairwise for all columns.\\n*   **Pros:** Relatively easy to implement. Provides a numerical measure of association.\\n*   **Cons:**  Pairwise calculations are *very* slow with 1000+ columns (n*(n-1)/2 calculations).  Doesn\\'t capture higher-order relationships (interactions between three or more columns).  Interpretation of Cramer\\'s V can be subjective.\\n\\n**3. Dimensionality Reduction Techniques (PCA-like, but for Categorical Data)**\\n\\n*   **How it works:** Techniques like Multiple Correspondence Analysis (MCA) are designed for categorical data. MCA finds linear combinations of categories to represent the data in a lower-dimensional space.\\n*   **Pros:** Handles categorical data directly.  Reduces dimensionality.\\n*   **Cons:** Interpretation can be less intuitive than some other methods. Might not capture all the nuances of the relationships.\\n\\n**4. Bayesian Networks**\\n\\n*   **How it works:** Bayesian networks learn a probabilistic graphical model representing the dependencies between variables.  They can infer relationships from the data.\\n*   **Pros:** Explicitly models dependencies. Can be used for prediction.\\n*   **Cons:** Can be computationally intensive, especially with many variables.  Structure learning (finding the network structure) can be challenging.\\n\\n**Recommendations & Workflow**\\n\\n1.  **Start with Autoencoders:** Given your existing interest, this is a good starting point. Experiment with different embedding sizes and architectures.\\n2.  **Consider MCA for Initial Exploration:** MCA can give you a quick overview of the data structure.\\n3.  **Parallelize Correlation Calculations:** If you go with correlation-based methods, use parallel processing to speed up the pairwise calculations.\\n4.  **Focus on Top Relationships:** With so many columns, you\\'ll likely want to focus on the relationships with the highest association scores (from autoencoders, Cramer\\'s V, etc.).\\n\\n\\n\\nTo help me narrow down the best approach, could you tell me:\\n\\n*   What\\'s the goal of understanding these relationships? (e.g., feature selection, anomaly detection, generating insights?)\\n*   What are your computational resources like?', additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:40:47.765868Z', 'done': True, 'done_reason': 'stop', 'total_duration': 53390633000, 'load_duration': 188829917, 'prompt_eval_count': 274, 'prompt_eval_duration': 540662417, 'eval_count': 712, 'eval_duration': 52216876160, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb3-8403-7643-915a-c70eaafd4544-0', usage_metadata={'input_tokens': 274, 'output_tokens': 712, 'total_tokens': 986})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"OK.. But I have a large dataset with over 1000 columns. All these are categorical data and I want to understand the relationship between these columns without doing much of data pre-processing. What is the best way to do this?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6d6b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You've raised a great point, Rohit! Using Decision Trees (or their ensemble variants like Random Forests and Gradient Boosting) is indeed a viable option, and it's particularly attractive given your desire to avoid extensive preprocessing of categorical features. Here's a breakdown of how you can leverage Decision Trees in this scenario:\\n\\n**How Decision Trees Handle Categorical Data**\\n\\nDecision Trees natively handle categorical features without requiring one-hot encoding or other transformations. The tree-building algorithm directly considers the categories as possible split points. This is a significant advantage.\\n\\n**Methods & Considerations**\\n\\n1.  **Single Decision Tree:** You could train a single decision tree to predict a target variable (even if it's just a dummy variable). The tree's structure will reveal which features are most important for splitting the data, thus highlighting relationships.\\n2.  **Random Forest:** A Random Forest is an ensemble of decision trees. It's generally more robust and accurate than a single tree. The feature importance scores provided by Random Forests are a direct measure of how much each feature contributes to reducing impurity (e.g., Gini impurity or entropy) across all the trees.\\n3.  **Gradient Boosting Machines (GBM):** Similar to Random Forests, GBMs are ensemble methods that sequentially build trees, correcting the errors of previous trees. They also provide feature importance scores. XGBoost, LightGBM, and CatBoost are popular and highly optimized GBM implementations. CatBoost is particularly noteworthy because it has built-in handling for categorical features and often requires less preprocessing.\\n\\n**Workflow & Interpretation**\\n\\n1.  **Feature Importance:** Train a Random Forest or GBM (especially CatBoost) and examine the feature importance scores. These scores will rank the columns based on their predictive power.\\n2.  **Partial Dependence Plots (PDPs):** PDPs can help you visualize the relationship between a single feature and the predicted outcome. This can reveal how different categories within a feature influence the predictions.\\n3.  **SHAP Values:** SHAP (SHapley Additive exPlanations) values provide a more granular explanation of individual predictions, attributing the contribution of each feature to the prediction.\\n\\n**Advantages of Decision Tree-Based Methods**\\n\\n*   **No Preprocessing:** Handles categorical data directly.\\n*   **Interpretability:** Feature importance scores and PDPs offer insights into relationships.\\n*   **Robustness:** Ensemble methods (Random Forest, GBM) are generally robust to noise and outliers.\\n*   **CatBoost's Native Categorical Handling:** CatBoost is specifically designed to handle categorical features efficiently.\\n\\n**Disadvantages**\\n\\n*   **Potential for Overfitting:** Decision trees can overfit the data, especially if they are deep. Ensemble methods mitigate this risk but still require careful tuning.\\n*   **Bias Towards Features with Many Categories:** Features with a large number of categories might be artificially favored by some algorithms.\\n\\n\\n\\nGiven your constraints, I'd strongly recommend starting with **CatBoost**. Its ability to handle categorical features natively and its generally good performance make it a very attractive option.\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:41:40.663068Z', 'done': True, 'done_reason': 'stop', 'total_duration': 52852823291, 'load_duration': 198304500, 'prompt_eval_count': 979, 'prompt_eval_duration': 6831634500, 'eval_count': 636, 'eval_duration': 45413745498, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb4-54c0-7891-88cc-9a908a4271c6-0', usage_metadata={'input_tokens': 979, 'output_tokens': 636, 'total_tokens': 1615})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"Thats very detailed approach. But is it possible to use Decision Tree in this sceanrio? Again I dont want to preprocess categorical data\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e41946c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don\\'t have access to real-time information, including the current weather. I\\'m a language model, and my knowledge is based on the data I was trained on, which isn\\'t updated live.\\n\\nTo find out the weather today, I recommend checking these resources:\\n\\n*   **A weather app on your phone:** Most smartphones have built-in weather apps.\\n*   **A weather website:** Popular options include Google Weather, AccuWeather, The Weather Channel, and more.\\n*   **A quick Google search:** Just type \"weather [your location]\" into Google.\\n\\n\\n\\nHope you have a pleasant day, whatever the weather!', additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:42:01.929671Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21199899042, 'load_duration': 204671708, 'prompt_eval_count': 1505, 'prompt_eval_duration': 10997241750, 'eval_count': 136, 'eval_duration': 9821680044, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb5-2372-73f3-adbc-aeed66aa3557-0', usage_metadata={'input_tokens': 1505, 'output_tokens': 136, 'total_tokens': 1641})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"Do you know what is weather today?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a93d85f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Okay, this is a great and increasingly important question! Getting precise numbers for quantized Gemma 3 running on an M1 Mac is tricky because performance varies *a lot* based on the specific quantization method, the M1 Mac\\'s configuration (memory, etc.), and the task being performed. However, I can give you a good overview and some realistic ranges based on what\\'s been observed by the community.\\n\\nHere\\'s a breakdown of what I know, combining information from various sources and community discussions:\\n\\n**Understanding the Variables**\\n\\n*   **Quantization Level:** This is the biggest factor. Common levels are:\\n    *   **Q4_K_M:**  A popular choice, offering a good balance of size and performance.\\n    *   **Q5_K_M:** Slightly better performance than Q4, but larger.\\n    *   **Q8_0:**  Closest to full precision (FP16), offering the best performance but the largest size.\\n*   **M1 Mac Configuration:**  M1, M1 Pro, M1 Max, and M1 Ultra all have different memory and GPU capabilities.  More memory is *always* better for larger models.\\n*   **Framework/Implementation:**  The software used to run the model (e.g., llama.cpp, MLC LLM) significantly impacts performance.  `llama.cpp` is generally considered very efficient for Apple Silicon.\\n*   **Task:** Inference speed (tokens/second) depends on the task.  Simple text generation is faster than complex reasoning or code generation.\\n\\n**Estimated Performance Ranges (llama.cpp)**\\n\\nI\\'ll focus on `llama.cpp` as it\\'s the most common and optimized framework for running quantized models on Apple Silicon.  These are *estimates* and can vary.  I\\'ll give ranges for tokens/second (higher is better).\\n\\n*   **Gemma 3B Q4_K_M on M1 (Base Model):**\\n    *   **Typical Range:** 15 - 35 tokens/second.  You might see lower numbers with longer contexts or more complex prompts.\\n    *   **Memory Usage:** Around 4-5 GB.\\n*   **Gemma 3B Q5_K_M on M1 (Base Model):**\\n    *   **Typical Range:** 12 - 28 tokens/second.  Slightly slower than Q4, but potentially better quality.\\n    *   **Memory Usage:** Around 5-6 GB.\\n*   **Gemma 3B Q8_0 on M1 (Base Model):**\\n    *   **Typical Range:** 10 - 22 tokens/second.  Closest to FP16 performance, but still slower than FP16 on a dedicated GPU.\\n    *   **Memory Usage:** Around 7-8 GB.\\n*   **Gemma 3B on M1 Pro/Max:**  You\\'ll see significantly better performance on these models due to more memory and GPU cores.  Expect token generation speeds to be 20-50% higher than on a base M1.\\n\\n**Important Notes & Resources**\\n\\n*   **Context Length:** Longer context lengths (the amount of text the model considers when generating the next token) *drastically* reduce performance.\\n*   **Threads:** `llama.cpp` allows you to specify the number of threads to use. Experiment to find the optimal number for your M1 Mac.\\n*   **MLC LLM:** MLC LLM is another option, but generally slower than `llama.cpp` on Apple Silicon.\\n*   **Community Benchmarks:** Search online forums (Reddit\\'s r/LocalLLaMA is a great resource) and GitHub repositories for more specific benchmarks from other M1 Mac users.  Look for posts mentioning \"Gemma 3B\" and \"llama.cpp.\"\\n*   **Experimentation is Key:** The best way to know for sure is to try it yourself! Download `llama.cpp`, quantize the Gemma 3B model, and run some tests.\\n\\n\\n\\nTo help me narrow down the estimate, could you tell me:\\n\\n*   What specific M1 Mac do you have (M1, M1 Pro, M1 Max, M1 Ultra)?\\n*   What quantization level are you planning to use?\\n*   What task are you primarily interested in (e.g., simple chat, code generation)?', additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:43:18.860733Z', 'done': True, 'done_reason': 'stop', 'total_duration': 76879769875, 'load_duration': 197265875, 'prompt_eval_count': 896, 'prompt_eval_duration': 6299764084, 'eval_count': 942, 'eval_duration': 69756043831, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb5-7676-7050-a7c8-24bb3cf8c252-0', usage_metadata={'input_tokens': 896, 'output_tokens': 942, 'total_tokens': 1838})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"Do you know how much quantized is gemma3 model running on M1 mac?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb110f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As a large language model, I have no memory of past conversations. Each interaction we have is fresh. I don't retain information from previous exchanges. \\n\\nSo, I don't know what you were working on before this conversation. You'll need to refer to your own records or notes to recall that.\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:43:32.533647Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13616170667, 'load_duration': 222820292, 'prompt_eval_count': 1181, 'prompt_eval_duration': 8484402459, 'eval_count': 66, 'eval_duration': 4843749377, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb6-a2fe-7200-aa34-30e235346536-0', usage_metadata={'input_tokens': 1181, 'output_tokens': 66, 'total_tokens': 1247})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"What was I working on?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d62a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history called with session=id_123 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You haven't told me your name! I'm Gamma, your helpful assistant. ðŸ˜Š What's your name?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:43:42.63235Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10059769666, 'load_duration': 240380083, 'prompt_eval_count': 1108, 'prompt_eval_duration': 7956846417, 'eval_count': 26, 'eval_duration': 1810500460, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb6-d85a-7990-aafb-99f76dbca3fc-0', usage_metadata={'input_tokens': 1108, 'output_tokens': 26, 'total_tokens': 1134})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_trimmed_history.invoke(\n",
    "    {\"query\": \"What my name again?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493167b",
   "metadata": {},
   "source": [
    "```\n",
    "this is one of the drawback of using window buffered memory technique. some of the inforamtion gets lost. But keeping all these inforamtion is going to cause latency and increase the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0aa5ec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_123': BufferedWindowMessageHistory(messages=[HumanMessage(content='What was I working on?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"As a large language model, I have no memory of past conversations. Each interaction we have is fresh. I don't retain information from previous exchanges. \\n\\nSo, I don't know what you were working on before this conversation. You'll need to refer to your own records or notes to recall that.\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:43:32.533647Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13616170667, 'load_duration': 222820292, 'prompt_eval_count': 1181, 'prompt_eval_duration': 8484402459, 'eval_count': 66, 'eval_duration': 4843749377, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb6-a2fe-7200-aa34-30e235346536-0', usage_metadata={'input_tokens': 1181, 'output_tokens': 66, 'total_tokens': 1247}), HumanMessage(content='What my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You haven't told me your name! I'm Gamma, your helpful assistant. ðŸ˜Š What's your name?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:43:42.63235Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10059769666, 'load_duration': 240380083, 'prompt_eval_count': 1108, 'prompt_eval_duration': 7956846417, 'eval_count': 26, 'eval_duration': 1810500460, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fb6-d85a-7990-aafb-99f76dbca3fc-0', usage_metadata={'input_tokens': 1108, 'output_tokens': 26, 'total_tokens': 1134})], k=4)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaac5b7",
   "metadata": {},
   "source": [
    "### 3. ConversationSummaryMemory with RunnableWithMessageHistory\n",
    "store the summary of conversation in message lookup. Here summary is also generated from the LLM \n",
    "side effect to this approach is - there are far more tokens used compared to Window Buffered technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a4d7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages : list[BaseMessage] = Field(default_factory=list)\n",
    "    llm : ChatOllama = Field(default_factory=ChatOllama)\n",
    "\n",
    "    def __init__(self, llm:ChatOpenAI):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, message:list[BaseMessage]) -> None:\n",
    "        \"\"\" create a summary \"\"\"\n",
    "        self.messages.extend(message)\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Given the existing conversation summary and the new message, \n",
    "                generate a new summary of the conversation.\n",
    "                Ensure to maintain as much relevant information as possible.\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Existing Conversation Summary:\n",
    "                {existing_summary}\n",
    "\n",
    "                New Messages: \n",
    "                {messages}\"\"\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        new_summary = self.llm.invoke(summary_prompt.format_messages(existing_summary=self.messages, \n",
    "                                                                     messages=message))\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\" clear the history \"\"\" \n",
    "        self.messages = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52091c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id:str, llm:ChatOllama) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    \n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8696d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline, \n",
    "    get_session_history=get_chat_history, \n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\", \n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\", annotation=\"str\", name=\"Session ID\", \n",
    "            description=\"The session id to use for chat history\", \n",
    "            default=\"id_default\"\n",
    "        ), \n",
    "\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\", annotation=ChatOllama, name=\"LLM\",\n",
    "            description=\"LLM to use for the conversation history\",\n",
    "            default=ollama_llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76c09e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit! It's nice to meet you. I'm Gamma, your helpful assistant. What can I do for you today?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:50:54.695632Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12229472333, 'load_duration': 4737116875, 'prompt_eval_count': 59, 'prompt_eval_duration': 5334354333, 'eval_count': 29, 'eval_duration': 1989334336, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fbd-6798-74a0-a43e-be21ed0e8075-0', usage_metadata={'input_tokens': 59, 'output_tokens': 29, 'total_tokens': 88})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi my name is Rohit\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"llm\": ollama_llm}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "918f9c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Rohit! Happy to help.\\n\\nAutoencoders are a type of neural network used for unsupervised learning. They learn to compress and then reconstruct input data. Essentially, they have an encoder that reduces the data to a lower-dimensional \"latent space\" representation, and a decoder that reconstructs the original data from this compressed form. They\\'re often used for dimensionality reduction, feature extraction, and anomaly detection.\\n\\n\\n\\nLet me know if you\\'d like to explore a specific aspect of autoencoders!', additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T14:52:42.946281Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13111111250, 'load_duration': 216455833, 'prompt_eval_count': 101, 'prompt_eval_duration': 5646838083, 'eval_count': 101, 'eval_duration': 7171665544, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fbf-0afe-7863-8935-60d37101c986-0', usage_metadata={'input_tokens': 101, 'output_tokens': 101, 'total_tokens': 202})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I am working on autoencoders. Can you tell me briefly about autoencoders in less than 100 words?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"llm\": ollama_llm}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3fcf186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationSummaryMessageHistory(messages=[SystemMessage(content='Rohit introduced himself and Gamma, a helpful assistant, acknowledged him and offered assistance. Rohit then inquired about autoencoders, and Gamma briefly explained that they are a type of neural network used for unsupervised learning, compressing and reconstructing data through an encoder and decoder. They are commonly used for dimensionality reduction, feature extraction, and anomaly detection, and Gamma offered to explore specific aspects further.', additional_kwargs={}, response_metadata={})], llm=ChatOllama(model='gemma3:12b', temperature=0.0))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30154b0b",
   "metadata": {},
   "source": [
    "### 4. ConversationSummaryBufferHistory using RunnableWithMessageHistory\n",
    "Store the latest information and summary details opposed to everything in summary. This will have detailed inforamtion of latest information and also will reduce the context window size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a86a362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferedMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages : list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOllama = Field(default_factory=ChatOllama)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm:ChatOllama, k:int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "    \n",
    "    def add_messages(self, messages:list[BaseMessage]) -> None:\n",
    "        \"\"\" Add messages to history, removing any messages beyond the last `k` messages and summarizing the messages we dropped \"\"\"\n",
    "        existing_summary:SystemMessage|None = None \n",
    "        old_messages=None \n",
    "\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        if len(self.messages) > self.k:\n",
    "            print(f\" >> Found {len(self.messages)} messages, Dropping {len(self.messages) - self.k} messages.\")\n",
    "            old_messages = self.messages[:self.k]\n",
    "            self.messages = self.messages[-self.k:] \n",
    "        \n",
    "        if old_messages is None:\n",
    "            print (\" >> No old message to update summary with\")\n",
    "            return \n",
    "        \n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Given the existing conversation summary and the new message, \n",
    "                generate a new summary of the conversation.\n",
    "                Ensure to maintain as much relevant information as possible.\"\"\"                \n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Existing Conversation Summary:\n",
    "                {existing_summary}\n",
    "\n",
    "                New Messages: \n",
    "                {old_messages}\"\"\"                \n",
    "            )\n",
    "        ])\n",
    "\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(existing_summary=existing_summary, old_messages=old_messages)\n",
    "        )\n",
    "\n",
    "        print(f\" >> New Summary: {new_summary.content}\")\n",
    "\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None: \n",
    "        \"\"\" clear the history \"\"\"\n",
    "        self.messages = [] \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b870b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id:str, llm:ChatOllama, k:int) -> ConversationSummaryBufferedMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryBufferedMessageHistory(llm=llm, k=k)\n",
    "    \n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81a0e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline, \n",
    "    get_session_history=get_chat_history, \n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\", \n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\", annotation=\"str\", name=\"Session ID\", \n",
    "            description=\"The session id to use for chat history\", \n",
    "            default=\"id_default\"\n",
    "        ), \n",
    "\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\", annotation=ChatOllama, name=\"LLM\",\n",
    "            description=\"LLM to use for the conversation history\",\n",
    "            default=ollama_llm,\n",
    "        ),\n",
    "\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\", annotation=int, name=\"k\",\n",
    "            description=\"Number of messages to keep in history\", \n",
    "            default=4\n",
    "        )\n",
    "        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f467a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> No old message to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit! It's nice to meet you. I'm Gamma, your helpful assistant. What can I do for you today?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T15:39:26.855384Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7047462667, 'load_duration': 329409083, 'prompt_eval_count': 59, 'prompt_eval_duration': 4706900000, 'eval_count': 29, 'eval_duration': 1987341793, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fe9-eb76-73d3-8ff4-89906ff1991e-0', usage_metadata={'input_tokens': 59, 'output_tokens': 29, 'total_tokens': 88})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi my name is Rohit\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"llm\": ollama_llm, \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f69f1534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> No old message to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit!\\n\\nAn **encoder** is a neural network component that transforms data into a compressed representation (like squeezing information). Think of it as summarizing. \\n\\nAn **autoencoder** *uses* an encoder! It's a full neural network built with an encoder *and* a decoder. The encoder compresses, and the decoder tries to reconstruct the original data from that compressed form. Autoencoders learn efficient data representations.\\n\\n\\n\\nHope that helps!\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T15:39:34.068364Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7197659375, 'load_duration': 206439417, 'prompt_eval_count': 121, 'prompt_eval_duration': 506304000, 'eval_count': 92, 'eval_duration': 6434379335, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fea-0715-7650-bcac-299fca7e4912-0', usage_metadata={'input_tokens': 121, 'output_tokens': 92, 'total_tokens': 213})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is encoder and how is this different from autoencoders? Please explain in less than 100 words to me\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"llm\": ollama_llm, \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eba7c431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Found 6 messages, Dropping 2 messages.\n",
      " >> New Summary: Rohit introduced himself, and Gamma, an AI assistant, responded and offered help. Rohit then asked for a concise explanation of encoders and autoencoders. Gamma explained that an encoder compresses data, while an autoencoder uses an encoder and decoder to reconstruct data, learning efficient representations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit!\\n\\nThe sky is blue because of a phenomenon called **Rayleigh scattering**. Sunlight is made up of all colors, but blue and violet light have shorter wavelengths. When sunlight hits the Earth's atmosphere, these shorter wavelengths are scattered more by the tiny air molecules than other colors. That scattered blue light is what we see!\\n\\n\\n\\nI hope that explanation makes sense!\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T15:39:40.032784Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5949041916, 'load_duration': 188351250, 'prompt_eval_count': 226, 'prompt_eval_duration': 286952583, 'eval_count': 77, 'eval_duration': 5424217167, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fea-2342-7153-bd96-cfaefeb6a2f9-0', usage_metadata={'input_tokens': 226, 'output_tokens': 77, 'total_tokens': 303})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Why is sky blue\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"llm\": ollama_llm, \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a74593ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Found existing summary\n",
      " >> Found 6 messages, Dropping 2 messages.\n",
      " >> New Summary: Rohit initiated a conversation with Gamma, an AI assistant, and requested explanations of encoders and autoencoders. Gamma initially explained that an encoder compresses data and an autoencoder uses an encoder and decoder to reconstruct data and learn efficient representations. Following this, Gamma elaborated further on the difference between encoders and autoencoders, explaining that an autoencoder utilizes an encoder to compress data and a decoder to reconstruct it. Rohit then shifted the conversation to a new topic, asking why the sky is blue, to which Gamma responded with an explanation involving Rayleigh scattering.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Rohit!\\n\\nChoosing the right autoencoder depends on your specific task and data. Here's a breakdown of common types and when you might use them:\\n\\n*   **Vanilla Autoencoder:** A good starting point for basic dimensionality reduction and feature learning. Simple to understand and implement.\\n*   **Sparse Autoencoder:** Encourages sparsity in the hidden layer activations, leading to more interpretable features. Useful when you want to identify the most important features.\\n*   **Denoising Autoencoder:** Trained to reconstruct clean data from noisy input. Great for robust feature learning and handling noisy datasets.\\n*   **Variational Autoencoder (VAE):** Generates new data similar to the training data. Useful for generative tasks like image generation.\\n*   **Contractive Autoencoder:** Encourages the learned representation to be insensitive to small changes in the input. Good for learning robust features.\\n\\nTo help me narrow it down, could you tell me:\\n\\n*   What kind of data are you working with?\\n*   What are you hoping to achieve with the autoencoder?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T15:40:07.771893Z', 'done': True, 'done_reason': 'stop', 'total_duration': 18093564333, 'load_duration': 222073917, 'prompt_eval_count': 332, 'prompt_eval_duration': 2479700750, 'eval_count': 222, 'eval_duration': 15273209700, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fea-602c-7432-8aac-e32c6b20c6cd-0', usage_metadata={'input_tokens': 332, 'output_tokens': 222, 'total_tokens': 554})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Which autoencoder I should use?\"}, \n",
    "    config={\"configurable\" : {\"session_id\": \"id_123\", \"llm\": ollama_llm, \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f98fc28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationSummaryBufferedMessageHistory(messages=[SystemMessage(content='Rohit initiated a conversation with Gamma, an AI assistant, and requested explanations of encoders and autoencoders. Gamma initially explained that an encoder compresses data and an autoencoder uses an encoder and decoder to reconstruct data and learn efficient representations. Following this, Gamma elaborated further on the difference between encoders and autoencoders, explaining that an autoencoder utilizes an encoder to compress data and a decoder to reconstruct it. Rohit then shifted the conversation to a new topic, asking why the sky is blue, to which Gamma responded with an explanation involving Rayleigh scattering.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Why is sky blue', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Rohit!\\n\\nThe sky is blue because of a phenomenon called **Rayleigh scattering**. Sunlight is made up of all colors, but blue and violet light have shorter wavelengths. When sunlight hits the Earth's atmosphere, these shorter wavelengths are scattered more by the tiny air molecules than other colors. That scattered blue light is what we see!\\n\\n\\n\\nI hope that explanation makes sense!\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T15:39:40.032784Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5949041916, 'load_duration': 188351250, 'prompt_eval_count': 226, 'prompt_eval_duration': 286952583, 'eval_count': 77, 'eval_duration': 5424217167, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fea-2342-7153-bd96-cfaefeb6a2f9-0', usage_metadata={'input_tokens': 226, 'output_tokens': 77, 'total_tokens': 303}), HumanMessage(content='Which autoencoder I should use?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Rohit!\\n\\nChoosing the right autoencoder depends on your specific task and data. Here's a breakdown of common types and when you might use them:\\n\\n*   **Vanilla Autoencoder:** A good starting point for basic dimensionality reduction and feature learning. Simple to understand and implement.\\n*   **Sparse Autoencoder:** Encourages sparsity in the hidden layer activations, leading to more interpretable features. Useful when you want to identify the most important features.\\n*   **Denoising Autoencoder:** Trained to reconstruct clean data from noisy input. Great for robust feature learning and handling noisy datasets.\\n*   **Variational Autoencoder (VAE):** Generates new data similar to the training data. Useful for generative tasks like image generation.\\n*   **Contractive Autoencoder:** Encourages the learned representation to be insensitive to small changes in the input. Good for learning robust features.\\n\\nTo help me narrow it down, could you tell me:\\n\\n*   What kind of data are you working with?\\n*   What are you hoping to achieve with the autoencoder?\", additional_kwargs={}, response_metadata={'model': 'gemma3:12b', 'created_at': '2025-12-30T15:40:07.771893Z', 'done': True, 'done_reason': 'stop', 'total_duration': 18093564333, 'load_duration': 222073917, 'prompt_eval_count': 332, 'prompt_eval_duration': 2479700750, 'eval_count': 222, 'eval_duration': 15273209700, 'logprobs': None, 'model_name': 'gemma3:12b', 'model_provider': 'ollama'}, id='lc_run--019b6fea-602c-7432-8aac-e32c6b20c6cd-0', usage_metadata={'input_tokens': 332, 'output_tokens': 222, 'total_tokens': 554})], llm=ChatOllama(model='gemma3:12b', temperature=0.0), k=4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74981152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generativeai-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
